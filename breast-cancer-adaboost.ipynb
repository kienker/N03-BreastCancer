{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"**A Comparison of Logistic Regression and Random Forest with Dimension Reduction for\nBreast Cancer Classification**\n(Accuracy=0.991 Sensitivity=0.977 Specificity=1.000 Precision=1.000)\n\n* **1. Load libraries and clean data**\n\n* **2. Data analysis**\n\n * **2. 1. Descriptive  Statistics**\n\n * **2. 2. Outlier detection**\n\n * **2. 3. Target distribution**\n\n * **2. 4. Selected Joint and marginal Feature distributions**\n\n * **2. 5. Correlation matrix**\n\n* **3. Data preprocessing and Feature Engineering**\n\n * **3. 1. Encoding labels**\n\n * **3. 2. Data split**\n\n * **3. 3. Feature Engineering**\n\n    * **3. 3. 1. Scaler**\n\n    * **3. 3. 2. Dimensionality reduction**\n\n      * **Principal component analysis (PCA)**\n\n      * **Recursive features elimination(RFE)**\n\n* **4. Grid Search Cross validation**\n\n * **4. 1. Find the best hyperparameters**\n\n * **4. 2. # of Components in PCA versus Model Accuracy/Training Time** \n\n* **5. Model Measures**\n\n * **5. 1. Confusion Matrices & Metrics**\n\n * **5. 2. ROC curves (AUC)**\n\n * **5. 3 Precision-recall curves**\n\n* **6. Model  comparison**\n\n * **6. 1. logisitic Regression**\n\n * **6. 2. logistic Regression with PCA**\n\n    * **6. 2. 1. # of Components in PCA versus Model Accuracy/Training Time**\n\n    * **6. 2. 2. logistic Regression with PCA (8 components)**\n    (threshold = 0.5 : Accuracy=0.991 Sensitivity=0.977 Specificity=1.000 Precision=1.000)\n\n    * **6. 2. 3. Adjusting thresholds for metrics**\n\n * **6. 3. Random Forest**\n\n * **6. 4. Random Forest with PCA**\n\n    * **6. 4. 1. # of Components in PCA versus Model Accuracy/Training Time**\n\n    * **6. 4. 2. logistic Regression with PCA (14 components)**\n\n * **6. 5. Random Forest with RFE ( Recursive features elimination)**\n\n * **6. 6. Model Performance Plot**","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.  Load libraries and clean data**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, roc_curve, precision_recall_curve, auc\nfrom plotly.subplots import make_subplots\nimport itertools\n# Run the following two lines of code for Uncaught Error: Script error for plotly\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:24:58.316811Z","iopub.execute_input":"2022-04-07T17:24:58.31716Z","iopub.status.idle":"2022-04-07T17:25:01.136292Z","shell.execute_reply.started":"2022-04-07T17:24:58.317071Z","shell.execute_reply":"2022-04-07T17:25:01.135453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:25:10.613752Z","iopub.execute_input":"2022-04-07T17:25:10.614162Z","iopub.status.idle":"2022-04-07T17:25:10.672074Z","shell.execute_reply.started":"2022-04-07T17:25:10.61413Z","shell.execute_reply":"2022-04-07T17:25:10.671257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values_count = df.isnull().sum()\nmissing_values_count","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:27:27.659846Z","iopub.execute_input":"2022-04-07T17:27:27.660148Z","iopub.status.idle":"2022-04-07T17:27:27.668729Z","shell.execute_reply.started":"2022-04-07T17:27:27.660119Z","shell.execute_reply":"2022-04-07T17:27:27.667998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['id','Unnamed: 32'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:27:38.773842Z","iopub.execute_input":"2022-04-07T17:27:38.77414Z","iopub.status.idle":"2022-04-07T17:27:38.800689Z","shell.execute_reply.started":"2022-04-07T17:27:38.774106Z","shell.execute_reply":"2022-04-07T17:27:38.799597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Data analysis**\n\n**2. 1. Descriptive  Statistics**","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:27:41.892186Z","iopub.execute_input":"2022-04-07T17:27:41.892474Z","iopub.status.idle":"2022-04-07T17:27:41.898686Z","shell.execute_reply.started":"2022-04-07T17:27:41.892445Z","shell.execute_reply":"2022-04-07T17:27:41.897734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.diagnosis.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:27:44.460839Z","iopub.execute_input":"2022-04-07T17:27:44.461104Z","iopub.status.idle":"2022-04-07T17:27:44.470716Z","shell.execute_reply.started":"2022-04-07T17:27:44.461076Z","shell.execute_reply":"2022-04-07T17:27:44.469999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:27:58.727084Z","iopub.execute_input":"2022-04-07T17:27:58.727407Z","iopub.status.idle":"2022-04-07T17:27:58.809751Z","shell.execute_reply.started":"2022-04-07T17:27:58.727366Z","shell.execute_reply":"2022-04-07T17:27:58.809078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. 2. Outlier detection**","metadata":{}},{"cell_type":"code","source":"names = df.columns[5:11]\n# convert DataFrame to list\nvalues=[] \nfor column in df.iloc[:,5:11].columns:\n    li = df[column].tolist()\n    values.append(li)\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen','cyan', 'royalblue']\n\nfig = go.Figure()\nfor xd, yd, cls in zip(names, values, colors):\n        fig.add_trace(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints='outliers',\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker_size=3,\n            line_width=2)\n        )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:28:13.674721Z","iopub.execute_input":"2022-04-07T17:28:13.675167Z","iopub.status.idle":"2022-04-07T17:28:13.813872Z","shell.execute_reply.started":"2022-04-07T17:28:13.675121Z","shell.execute_reply":"2022-04-07T17:28:13.813022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. 3. Target distribution**","metadata":{}},{"cell_type":"code","source":"fig = go.Figure(data=[go.Pie(labels=['Benign','Malignant'], values=df['diagnosis'].value_counts(), textinfo='label+percent')])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=['gold', 'mediumturquoise'], line=dict(color='#000000', width=2)))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:28:33.29375Z","iopub.execute_input":"2022-04-07T17:28:33.294039Z","iopub.status.idle":"2022-04-07T17:28:33.346238Z","shell.execute_reply.started":"2022-04-07T17:28:33.294007Z","shell.execute_reply":"2022-04-07T17:28:33.345482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. 4. Selected Joint and marginal Feature distributions**","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df.iloc[:,:6],hue='diagnosis', diag_kind='hist',height=1.6)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:03:50.304835Z","iopub.execute_input":"2022-03-29T22:03:50.305484Z","iopub.status.idle":"2022-03-29T22:03:57.436221Z","shell.execute_reply.started":"2022-03-29T22:03:50.305448Z","shell.execute_reply":"2022-03-29T22:03:57.435304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. 5. Correlation matrix**","metadata":{}},{"cell_type":"code","source":"corr = df.iloc[:,1:].corr()\nfig = go.Figure(data=go.Heatmap(z=np.array(corr),x=corr.columns.tolist(),y=corr.columns.tolist(),xgap = 1,ygap = 1))\nfig.update_layout(margin = dict(t=25,r=0,b=200,l=200),width = 1000, height = 700)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:03:57.437803Z","iopub.execute_input":"2022-03-29T22:03:57.43825Z","iopub.status.idle":"2022-03-29T22:03:57.477819Z","shell.execute_reply.started":"2022-03-29T22:03:57.438212Z","shell.execute_reply":"2022-03-29T22:03:57.476972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Data preprocessing and Feature Engineering**\n\n**3. 1. Encoding labels**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['diagnosis'] = le.fit_transform(df['diagnosis']) # M:1, B:0\ndf['diagnosis'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:03:57.47882Z","iopub.execute_input":"2022-03-29T22:03:57.479011Z","iopub.status.idle":"2022-03-29T22:03:57.487881Z","shell.execute_reply.started":"2022-03-29T22:03:57.478987Z","shell.execute_reply":"2022-03-29T22:03:57.487006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. 2. Data split**","metadata":{}},{"cell_type":"code","source":"random_state = 42\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df['diagnosis'], test_size = 0.2, random_state = random_state)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:03:57.489672Z","iopub.execute_input":"2022-03-29T22:03:57.490252Z","iopub.status.idle":"2022-03-29T22:03:57.50045Z","shell.execute_reply.started":"2022-03-29T22:03:57.490208Z","shell.execute_reply":"2022-03-29T22:03:57.499597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. 3. Feature Engineering**\n\n**3. 3. 1. Scaler**\n\nIn meachine learning, many algorithms (expect some algorithms ,like decision trees and random forests) are sensitive to features with different scales, so it’s important to scale numerical input variables to a standard range. Normalizer (standard deviation scaling) and StandardScaler (mean removal and  standard deviation scaling) are the two most popular scaling techniques. In this case, we dectect in boxplots that some features contains a few extreme outlies which are not easy to remove or replace. Extreme outlies often influence the sample mean / variance in a negative way. Therefore, the above two scalers might not work very well here. We use **RobustScaler** as a replacement. This scaler is more robust to outliers by removing the median and scaling the data according to the quantile range.","metadata":{}},{"cell_type":"code","source":"scale = RobustScaler()\nX_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:03:57.502236Z","iopub.execute_input":"2022-03-29T22:03:57.502561Z","iopub.status.idle":"2022-03-29T22:03:57.520564Z","shell.execute_reply.started":"2022-03-29T22:03:57.502504Z","shell.execute_reply":"2022-03-29T22:03:57.51962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. 3. 2. Dimensionality reduction**\n\nFrom correlation matrix and Joint and marginal distributions plots, we observe that some features are highly correlated, like radius_mean, perimeter_mean and area_mean. Those features have almost the same effect on the dependent variable. In real world problem, classification algorithms including too many features in dataset commonly suffer from the \"Curse of Dimensionality\", which means more data, bulky computation, and also overfitting risk. Both feature selection (selecting specific features to include) and feature extraction(extracting a new feature set from the input features) can effectively avoid these problems. **In practice, the effectiveness of dimensionality reduction highly depends on the algorithm to be applied later and type of the data to be passed into the methodology.** Here, we will try two different methods: **Principal component analysis (PCA)** and **Recursive features elimination(RFE)**, and also compare the results by separately using them on classification algorithms. ","metadata":{}},{"cell_type":"markdown","source":"* **Principal component analysis (PCA)**","metadata":{}},{"cell_type":"code","source":"pca = PCA()\npca.fit(X_train)\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\nfig = px.line(x=np.arange(1,exp_var_cumul.shape[0]+1), y=exp_var_cumul, markers=True, labels={'x':'# of components', 'y':'Cumulative Explained Variance'})\n\n#fig.add_shape(type='line', line=dict(dash='dash'),x0=0, x1=30, y0=0.95, y1=0.95)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:03:57.52166Z","iopub.execute_input":"2022-03-29T22:03:57.521885Z","iopub.status.idle":"2022-03-29T22:03:58.399417Z","shell.execute_reply.started":"2022-03-29T22:03:57.52186Z","shell.execute_reply":"2022-03-29T22:03:58.398503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an elbow after the seventh component, and 91% of the total variance is explained by the firrst seventh components. If keeping the first 10 or 17 principal components, we can preserve about 95 % or even more than 99% of the total variance.","metadata":{}},{"cell_type":"markdown","source":"* **Recursive features elimination(RFE)**\n\nRecursive feature elimination (RFE) is a feature selection method. The process is that a separate model is trained and each time the weakest features are taken away, until the optimum set of features to use for the maximum results is reached. ","metadata":{}},{"cell_type":"code","source":"# Fit the RFE model to identify the optimum number of features .\nrfecv = RFECV(cv=StratifiedKFold(n_splits=10, random_state=random_state, shuffle=True),\n      estimator=DecisionTreeClassifier(), scoring='accuracy')\n    \nrfecv.fit(X_train, y_train)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(15,5))\nplt.xlabel(\"# of features selected\")\nplt.ylabel(\"Cross validation score (accuracy)\")\nplt.plot(\n    range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n    rfecv.cv_results_['mean_test_score'],\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:10:45.694332Z","iopub.execute_input":"2022-03-29T22:10:45.695037Z","iopub.status.idle":"2022-03-29T22:10:47.271503Z","shell.execute_reply.started":"2022-03-29T22:10:45.69498Z","shell.execute_reply":"2022-03-29T22:10:47.270464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualise how the model’s performance changes when the number of features increases by ploting the number of features used in the model versus the model accuracy. As you can see, more features isn’t always better. The accuracy value has reached the highest when the first # important features are used in the model.","metadata":{}},{"cell_type":"code","source":"# Identifying the features RFE selected\ndf_features = pd.DataFrame(columns = ['feature', 'support', 'ranking'])\n\nfor i in range(X_train.shape[1]):\n    row = {'feature': i, 'support': rfecv.support_[i], 'ranking': rfecv.ranking_[i]}\n    df_features = df_features.append(row, ignore_index=True)\n    \ndf_features.sort_values(by='ranking').head(10) #df_features[df_features['support']==True]","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:10:51.6886Z","iopub.execute_input":"2022-03-29T22:10:51.688877Z","iopub.status.idle":"2022-03-29T22:10:51.791719Z","shell.execute_reply.started":"2022-03-29T22:10:51.688849Z","shell.execute_reply":"2022-03-29T22:10:51.790794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying the features' name RFE selected\ndf.columns[1:][rfecv.get_support()]","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:10:55.347704Z","iopub.execute_input":"2022-03-29T22:10:55.347982Z","iopub.status.idle":"2022-03-29T22:10:55.354503Z","shell.execute_reply.started":"2022-03-29T22:10:55.347954Z","shell.execute_reply":"2022-03-29T22:10:55.353846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Grid Search Cross validation**\n\n**4. 1. Find the best hyperparameters**","metadata":{}},{"cell_type":"code","source":"def modelselection(classifier, parameters, scoring, X_train):\n    clf = GridSearchCV(estimator=classifier,\n                   param_grid=parameters,\n                   scoring= scoring,\n                   cv=5,\n                   n_jobs=-1)# n_jobs refers to the number of CPU's that you want to use for excution, -1 means that use all available computing power.\n    clf.fit(X_train, y_train)\n    cv_results = clf.cv_results_\n    best_parameters = clf.best_params_\n    best_result = clf.best_score_\n    print('The best parameters for classifier is', best_parameters)\n    print('The best training score is %.3f:'% best_result)\n#    print(sorted(cv_results.keys()))\n    return cv_results, best_parameters, best_result","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:00.058472Z","iopub.execute_input":"2022-03-29T22:04:00.058723Z","iopub.status.idle":"2022-03-29T22:04:00.068411Z","shell.execute_reply.started":"2022-03-29T22:04:00.058686Z","shell.execute_reply":"2022-03-29T22:04:00.067506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. 2. # of Components in PCA versus Model Accuracy/Training Time** ","metadata":{}},{"cell_type":"code","source":"def PCA_curves(PCA_cv_score, PCA_test_score, training_time):\n    fig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n    subplot_titles=('# of Components in PCA versus Model Accuracy','# of Components in PCA versus Training Time')\n    )\n    \n    fig.add_trace(go.Scatter(x=n,y=PCA_cv_score,\n                             line=dict(color='rgb(231,107,243)', width=2), name='CV score'),\n                  row=1, col=1)\n    fig.add_trace(go.Scatter(x=n,y=PCA_test_score,\n                             line=dict(color='rgb(0,176,246)', width=2), name='Test score'),              \n                  row=1, col=1)    \n    fig.add_trace(go.Scatter(x=n,y=training_time,\n                             line=dict(color='rgb(0,100,80)', width=2), name='Training time'),\n                  row=1, col=2)\n    fig.update_xaxes(title_text='# of components')\n    fig.update_yaxes(title_text='Accuracy', row=1, col=1)\n    # fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n    fig.update_yaxes(title_text='Training time', row=1, col=2)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:00.070103Z","iopub.execute_input":"2022-03-29T22:04:00.07092Z","iopub.status.idle":"2022-03-29T22:04:00.081533Z","shell.execute_reply.started":"2022-03-29T22:04:00.070873Z","shell.execute_reply":"2022-03-29T22:04:00.080897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. Model Measures**\n\n**5. 1. Confusion Matrices & Metrics**\n\nA confusion matrix is a table that categorizes predictions according to whether they match the actual value\n* True Positive (TP):  Malignant tumour correctly classified as Malignant\n* True Negative (TN): Benign tumour correctly classified as benign\n* False Positive (FP): Benign tumour incorrectly classified as malignant\n* False Negative (FN): Malignant tumour incorrectly classified as benign\n\nMetrics\n* Accuracy (called the success rate): (tp+tn)/(tp+tn+fp+fn)\n* Sensitivity (also called the true positive rate): tp/(tp+fn)\n* Specificity (also called the true negative rate): tn/(tn+fp)\n* Precision (also called the positive predictive value): tp/(tp+fp)\n* Recall: as the same as sensitivity\n* F-measure: 2*Precision*Recall/(Recall+Precision) = 2tp/(2tp+fp+fn)","metadata":{}},{"cell_type":"code","source":"def metrics(X,CV_clf):\n    y_pred = CV_clf.predict(X)\n    cm = confusion_matrix(y_test, y_pred)\n    tn = cm[0,0]\n    fp = cm[0,1]\n    fn = cm[1,0]\n    tp = cm[1,1]\n    Accuracy=(tp+tn)/(tp+tn+fp+fn)\n    Sensitivity=tp/(tp+fn)\n    Specificity=tn/(tn+fp)\n    Precision=tp/(tp+fp)\n    F_measure=2*tp/(2*tp+fp+fn)\n    print('Accuracy=%.3f'%Accuracy)\n    print('Sensitivity=%.3f'%Sensitivity) # as the same as recall\n    print('Specificity=%.3f'%Specificity)\n    print('Precision=%.3f'%Precision)\n    print('F-measure=%.3f'%F_measure)\n    return Accuracy, Sensitivity, Specificity, Precision, F_measure\n #   plot_confusion_matrix(CV_clf, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:00.082424Z","iopub.execute_input":"2022-03-29T22:04:00.082662Z","iopub.status.idle":"2022-03-29T22:04:00.097755Z","shell.execute_reply.started":"2022-03-29T22:04:00.082635Z","shell.execute_reply":"2022-03-29T22:04:00.096827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. 2. ROC curves (AUC)**\n\n* ROC curves: \n\nSummarize the trade-off between the true positive rate (sensitivity/recall) and false positive rate (1 – specificity) for a predictive model using different probability thresholds.\n\nused when datasets for each class are roughly equal.\n\nA classifier with no predictive value: a curve falls close to the diagonal line.\n\nA perfect classifier: a curve passes through the point at a 100% true positive rate and 0% false positive rate.\n\nAUC: measure the area under the ROC curve, range from 0.5 (for a classifier with no predictive value) to 1.0 (for a perfect classifier).","metadata":{}},{"cell_type":"markdown","source":"**5. 3 Precision-recall curves**\n\nSummarize the trade-off between the true positive rate (sensitivity/recall) and the positive predictive value (precision) for a predictive model using different probability thresholds.\n\nused when datasets for each class are moderate to large imbalance.\n\nA perfect classifier: a curve bows towards (1,1) and above the flat line (y=0.5) of no skill.","metadata":{}},{"cell_type":"code","source":"def plot_roc_prc():\n    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n    precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n    fig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n        subplot_titles=(f'ROC Curve (AUC={auc(fpr, tpr):.4f})',f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})')\n    )\n    fig.add_trace(go.Scatter(x=fpr, y=tpr),row=1, col=1)\n    fig.add_shape(type='line', line=dict(dash='dash'),x0=0, x1=1, y0=0, y1=1,row=1, col=1)\n    fig.add_trace(go.Scatter(x=recall, y=precision),row=1, col=2)\n    fig.add_shape(type='line', line=dict(dash='dash'),x0=0, x1=1, y0=0.5, y1=0.5,row=1, col=2)\n    # Update axis properties\n    fig.update_xaxes(title_text=\"False Positive Rate / 1-Specificity\", row=1, col=1)\n    fig.update_yaxes(title_text=\"True Positive Rate / Recall\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Precision\", row=1, col=2)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:00.098859Z","iopub.execute_input":"2022-03-29T22:04:00.099467Z","iopub.status.idle":"2022-03-29T22:04:00.109997Z","shell.execute_reply.started":"2022-03-29T22:04:00.099432Z","shell.execute_reply":"2022-03-29T22:04:00.109273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. Model  comparison**\n\n**6. 1. logisitic Regression**","metadata":{}},{"cell_type":"code","source":"classifier_log = LogisticRegression(random_state=random_state,solver='lbfgs', max_iter=1000)\nparameters_log = {\n            'penalty' : ['l2'],  \n            'C' : [0.01, 0.1, 1, 10, 100]\n}\nscoring='accuracy'   # scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n# Find the best hyperparameters\ncv_results, best_param, best_result = modelselection(classifier_log,parameters_log, scoring, X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:00.110953Z","iopub.execute_input":"2022-03-29T22:04:00.111734Z","iopub.status.idle":"2022-03-29T22:04:01.948204Z","shell.execute_reply.started":"2022-03-29T22:04:00.111691Z","shell.execute_reply":"2022-03-29T22:04:01.947092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classifier with the best hyperparameters\nlogReg_clf = LogisticRegression(penalty = best_param['penalty'],\n                            C = best_param['C'],\n                            random_state=random_state)\nlogReg_clf.fit(X_train, y_train)\n\n# Metrics\nlogReg_metrics = metrics(X_test,logReg_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:01.950822Z","iopub.execute_input":"2022-03-29T22:04:01.951636Z","iopub.status.idle":"2022-03-29T22:04:01.996176Z","shell.execute_reply.started":"2022-03-29T22:04:01.951574Z","shell.execute_reply":"2022-03-29T22:04:01.995195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 2. logistic Regression with PCA**","metadata":{}},{"cell_type":"code","source":"def compare_pca(n_components):\n    cv_score, test_score, cv_training_time = [], [], []\n    for n in n_components:\n        print(\"The number of components in PCA is:%d \"% n)\n        pca = PCA(n_components=n, svd_solver=\"full\",random_state=random_state)\n        X_PCA_train = pca.fit_transform(X_train)\n        X_PCA_test = pca.transform(X_test)\n        # Model Selection\n        cv_results, best_param, best_result = modelselection(classifier_log,parameters_log, scoring, X_PCA_train)\n        training_time = np.mean(np.array(cv_results['mean_fit_time'])+np.array(cv_results['mean_score_time']))\n        cv_score.append(best_result)\n        cv_training_time.append(training_time)\n        CV_clf = LogisticRegression(penalty = best_param['penalty'],\n                                    C = best_param['C'],\n                                    random_state=random_state)\n        CV_clf.fit(X_PCA_train, y_train)\n        score = CV_clf.score(X_PCA_test, y_test)\n        test_score.append(score)\n    print(cv_score, test_score, cv_training_time)\n    return cv_score, test_score, cv_training_time","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:01.998338Z","iopub.execute_input":"2022-03-29T22:04:01.99918Z","iopub.status.idle":"2022-03-29T22:04:02.014556Z","shell.execute_reply.started":"2022-03-29T22:04:01.999119Z","shell.execute_reply":"2022-03-29T22:04:02.013406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = X_train.shape[1]\nn = np.arange(2, n_features+2, 2) \n\nPCA_cv_score, PCA_test_score, PCA_cv_training_time= compare_pca(n_components = n)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:02.022267Z","iopub.execute_input":"2022-03-29T22:04:02.026518Z","iopub.status.idle":"2022-03-29T22:04:06.222202Z","shell.execute_reply.started":"2022-03-29T22:04:02.026438Z","shell.execute_reply":"2022-03-29T22:04:06.221152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 2. 1. # of Components in PCA versus Model Accuracy/Training Time**","metadata":{}},{"cell_type":"code","source":"PCA_curves(PCA_cv_score,PCA_test_score,PCA_cv_training_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:06.229166Z","iopub.execute_input":"2022-03-29T22:04:06.23319Z","iopub.status.idle":"2022-03-29T22:04:06.333738Z","shell.execute_reply.started":"2022-03-29T22:04:06.233101Z","shell.execute_reply":"2022-03-29T22:04:06.332738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 2. 2. logistic Regression with PCA (8 components)**","metadata":{}},{"cell_type":"code","source":"i =PCA_test_score.index(max(PCA_test_score))\nprint('The best accuracy of logistic regression classifier is: %.3f'%  max(PCA_test_score)+', where the total number of components in PCA is {:.0f}'.format((i+1)*2))","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:06.338814Z","iopub.execute_input":"2022-03-29T22:04:06.339085Z","iopub.status.idle":"2022-03-29T22:04:06.344233Z","shell.execute_reply.started":"2022-03-29T22:04:06.339054Z","shell.execute_reply":"2022-03-29T22:04:06.343404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=(i+1)*2, svd_solver=\"full\",random_state=random_state)\nX_PCA_train = pca.fit_transform(X_train)\nX_PCA_test = pca.transform(X_test)\n# Model Selection\ncv_results, best_param, best_result = modelselection(classifier_log,parameters_log, scoring, X_PCA_train)\n\n# Classifier with the best hyperparameters\nlogReg_PCA = LogisticRegression(penalty = best_param['penalty'],\n                            C = best_param['C'],\n                            random_state=random_state)\nlogReg_PCA.fit(X_PCA_train, y_train)\n\n# Metrics\nlogReg_PCA_metrics = metrics(X_PCA_test,logReg_PCA)\n\n# ROC Curve & Precision-Recall Curves\ny_score = logReg_PCA.predict_proba(X_PCA_test)[:, 1] # predict probabilities\nplot_roc_prc()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:06.34568Z","iopub.execute_input":"2022-03-29T22:04:06.346146Z","iopub.status.idle":"2022-03-29T22:04:06.597735Z","shell.execute_reply.started":"2022-03-29T22:04:06.346105Z","shell.execute_reply":"2022-03-29T22:04:06.596788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can control the tradeoff between TPR and FPR by predecting probabilities and adjusting a threshold. If the threhold above α% indicates a malignant tumor, different values of α yield different final TPR and FPR values. For example, the probability in [0.0, 0.49] means a negative outcome (0) with default threshold of 0.5, but could get a positive outcome (1) if chooing a threshold of 0.2. How to choose the threhold depending on our main concern. In this case, we may be far more concerned with having high true positive rate (or low flase negative) than high true negative rate (or low false positive). A high true positive rate means that patients would get the early treatment as much as possible. Sometimes, we also need a relative lower false positive value for a balance, because incorrectly indentifying benign tumor as malignant will cause patients excessive mental stress.","metadata":{}},{"cell_type":"markdown","source":"**6. 2. 3. Adjusting thresholds for metrics**","metadata":{}},{"cell_type":"code","source":"thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n\nfor n, ax in zip(thresholds,axs.ravel()):\n    y_score = logReg_PCA.predict_proba(X_PCA_test)[:,1] > n\n    \n    cm = confusion_matrix(y_test, y_score)\n    \n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n\n    print('threshold = %s :'%n,\n          'Accuracy={:.3f}'.format((tp+tn)/(tp+tn+fp+fn)),\n          'Sensitivity={:.3f}'.format(tp/(tp+fn)),\n          'Specificity={:.3f}'.format(tn/(tn+fp)),\n          'Precision={:.3f}'.format(tp/(tp+fp)))\n    \n    im=ax.matshow(cm, cmap='Blues', alpha=0.7)\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        ax.text(j, i, cm[i, j], horizontalalignment = 'center')\n        \n    ax.set_ylabel('True label',fontsize=12)\n    ax.set_xlabel('Predicted label',fontsize=12)\n    ax.set_title('Threshold = %s'%n, fontsize=12)\n    fig.colorbar(im, ax=ax,orientation='vertical');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:06.598947Z","iopub.execute_input":"2022-03-29T22:04:06.599198Z","iopub.status.idle":"2022-03-29T22:04:08.704539Z","shell.execute_reply.started":"2022-03-29T22:04:06.59915Z","shell.execute_reply":"2022-03-29T22:04:08.703537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 3. Random Forest**","metadata":{}},{"cell_type":"code","source":"parameters_rf = {\n    'n_estimators': [20, 50, 100, 150, 200],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False],\n}\n# We can test values for other parameters, such as max_features, max_depth, max_leaf_nodes, to see if the accuracy further impoves or not\nscoring_rf = 'accuracy' \n\"\"\"\n    scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n\"\"\"\nclassifier_rf = RandomForestClassifier(random_state=random_state)\n\n# Find the best hyperparameters\ncv_results, best_param, best_result = modelselection(classifier_rf,parameters_rf, scoring_rf, X_train)\n\n# Classifier with the best hyperparameters\nrf_clf = RandomForestClassifier(n_estimators = best_param['n_estimators'],\n                                criterion = best_param['criterion'],\n                                bootstrap = best_param['bootstrap'],\n                                random_state=random_state)\nrf_clf.fit(X_train, y_train)\n\n# Metrics\nrf_metrics = metrics(X_test,rf_clf)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:08.705821Z","iopub.execute_input":"2022-03-29T22:04:08.706082Z","iopub.status.idle":"2022-03-29T22:04:17.541766Z","shell.execute_reply.started":"2022-03-29T22:04:08.706046Z","shell.execute_reply":"2022-03-29T22:04:17.540782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 4. Random Forest with PCA**","metadata":{}},{"cell_type":"code","source":"def compare_pca(n_components):\n    cv_score, test_score, cv_training_time = [], [], []\n    for n in n_components:\n        print(\"The number of components in PCA is:%d \"% n)\n        pca = PCA(n_components=n, svd_solver=\"full\",random_state=random_state)\n        X_PCA_train = pca.fit_transform(X_train)\n        X_PCA_test = pca.transform(X_test)\n        # Model Selection\n        cv_results, best_param, best_result = modelselection(classifier_rf,parameters_rf, scoring, X_PCA_train)\n        training_time = np.mean(np.array(cv_results['mean_fit_time'])+np.array(cv_results['mean_score_time']))\n        cv_score.append(best_result)\n        cv_training_time.append(training_time)\n        CV_clf = RandomForestClassifier(n_estimators = best_param['n_estimators'],\n                                        criterion = best_param['criterion'],\n                                        bootstrap = best_param['bootstrap'],\n                                        random_state=random_state)\n        CV_clf.fit(X_PCA_train, y_train)\n        score = CV_clf.score(X_PCA_test, y_test)\n        test_score.append(score)\n    print(cv_score, test_score, cv_training_time)\n    return cv_score, test_score, cv_training_time","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:17.542977Z","iopub.execute_input":"2022-03-29T22:04:17.543221Z","iopub.status.idle":"2022-03-29T22:04:17.551416Z","shell.execute_reply.started":"2022-03-29T22:04:17.543192Z","shell.execute_reply":"2022-03-29T22:04:17.550487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = X_train.shape[1]\nn = np.arange(2, n_features+2, 2) \nPCA_cv_score, PCA_test_score, PCA_cv_training_time= compare_pca(n_components = n)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:04:17.552697Z","iopub.execute_input":"2022-03-29T22:04:17.55291Z","iopub.status.idle":"2022-03-29T22:06:28.512699Z","shell.execute_reply.started":"2022-03-29T22:04:17.552885Z","shell.execute_reply":"2022-03-29T22:06:28.511663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 4. 1. # of Components in PCA versus Model Accuracy/Training Time**","metadata":{}},{"cell_type":"code","source":"PCA_curves(PCA_cv_score, PCA_test_score, PCA_cv_training_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:06:28.514612Z","iopub.execute_input":"2022-03-29T22:06:28.51524Z","iopub.status.idle":"2022-03-29T22:06:28.555474Z","shell.execute_reply.started":"2022-03-29T22:06:28.515204Z","shell.execute_reply":"2022-03-29T22:06:28.554613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 4. 2. logistic Regression with PCA (14 components)**","metadata":{}},{"cell_type":"code","source":"i =PCA_test_score.index(max(PCA_test_score))\npca = PCA(n_components=(i+2)*2, svd_solver=\"full\",random_state=random_state)\nX_PCA_train = pca.fit_transform(X_train)\nX_PCA_test = pca.transform(X_test)\n# Model Selection\ncv_results, best_param, best_result = modelselection(classifier_rf,parameters_rf, scoring, X_PCA_train)\nrf_PCA = RandomForestClassifier(n_estimators = best_param['n_estimators'],\n                                criterion = best_param['criterion'],\n                                bootstrap = best_param['bootstrap'],\n                                random_state=random_state)\nrf_PCA.fit(X_PCA_train, y_train)\n# Metrics\nrf_PCA_metrics = metrics(X_PCA_test,rf_PCA)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:06:28.556679Z","iopub.execute_input":"2022-03-29T22:06:28.556913Z","iopub.status.idle":"2022-03-29T22:06:37.557956Z","shell.execute_reply.started":"2022-03-29T22:06:28.556884Z","shell.execute_reply":"2022-03-29T22:06:37.556939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 5. Random Forest with RFE ( Recursive features elimination)**","metadata":{}},{"cell_type":"code","source":"X_train_selected = X_train[:,rfecv.get_support()]\nX_test_selected = X_test[:,rfecv.get_support()]\n\ncv_results, best_param, best_result = modelselection(classifier_rf,parameters_rf, scoring_rf, X_train_selected)\n\n# Classifier with the best hyperparameters\nrf_RFE = RandomForestClassifier(n_estimators = best_param['n_estimators'],\n                                criterion = best_param['criterion'],\n                                bootstrap = best_param['bootstrap'],\n                                random_state=random_state)\nrf_RFE.fit(X_train_selected, y_train)\n\n# Metrics\nrf_RFE_metrics = metrics(X_test_selected ,rf_RFE)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:11:12.557698Z","iopub.execute_input":"2022-03-29T22:11:12.558343Z","iopub.status.idle":"2022-03-29T22:11:19.46132Z","shell.execute_reply.started":"2022-03-29T22:11:12.558296Z","shell.execute_reply":"2022-03-29T22:11:19.460394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. 6. Model Performance Plot**","metadata":{}},{"cell_type":"code","source":"models_metrics = {'logisticRegression': [round(elem, 3) for elem in logReg_metrics], \n                 'logReg+PCA': [round(elem, 3) for elem in logReg_PCA_metrics],\n                 'RandomForest' : [round(elem, 3) for elem in rf_metrics],\n                 'Rand+PCA' : [round(elem, 3) for elem in rf_PCA_metrics],\n                 'Rand+RFE' : [round(elem, 3) for elem in rf_RFE_metrics]\n                }\nindex=['Accuracy','Sensitivity','Specificity','Precision', 'F-measure']\ndf_scores = pd.DataFrame(data = models_metrics, index=index)\nax = df_scores.plot(kind='bar', figsize = (15,6), ylim = (0.90, 1.02), \n                    color = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen','cyan'],\n                    rot = 0, title ='Models performance (test scores)',\n                    edgecolor = 'grey', alpha = 0.5)\nax.legend(loc='upper center', ncol=5, title=\"models\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T22:11:25.335816Z","iopub.execute_input":"2022-03-29T22:11:25.336187Z","iopub.status.idle":"2022-03-29T22:11:25.709261Z","shell.execute_reply.started":"2022-03-29T22:11:25.336149Z","shell.execute_reply":"2022-03-29T22:11:25.708344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As said before, the effectiveness of dimensionality reduction highly depends on the algorithm to be applied later and type of the data to be passed into the methodology. In this breast cancer dataset, the classification performance in logistic regression method is obviously improved after using PCA. In the meanwhile, we also noticed that when we use random forest technique as a classifier, the classification metrics are no overall improvement either with PCA or RFE as dimension reduction method, although the training time is decreased as the number of used features reduced.","metadata":{}}]}